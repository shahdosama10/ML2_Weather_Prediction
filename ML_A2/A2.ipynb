{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### load 'weather_forecast_data.csv' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('weather_forecast_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get copy from the original to preprocess\n",
    "\n",
    "df_pre = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to know the number of the rows\n",
    "print(f\"total records:\",len(df), \"\\n\")\n",
    "\n",
    "\n",
    "# to get the number of missing values in each column\n",
    "print(\"missing records in each column:\",\"\\n\")\n",
    "print(df_pre.isnull().sum())\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"Records with null values: \",\"\\n\")\n",
    "print(df_pre[df_pre.isnull().any(axis=1)])\n",
    "\n",
    "# according to the output there are missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Handle missing values with dropping them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dropped_nulls= df_pre.dropna()\n",
    "print(f\"total records without nulls:\",len(df_dropped_nulls), \"\\n\")\n",
    "\n",
    "df_dropped_nulls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Handle missing values with replacing them with Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the numerical features only because we can't get mean for categorical feature\n",
    "\n",
    "df_numerical_features_only=df_pre.select_dtypes(include=\"number\")\n",
    "\n",
    "\n",
    "# replace the null values with the average of the numerical features\n",
    "\n",
    "df_numerical_filled_avg = df_numerical_features_only.fillna(df_numerical_features_only.mean())\n",
    "\n",
    "\n",
    "# concatenate the numerical features with the target column \"Rain\" and create a new dataframe \"df_filledAvg\"\n",
    "\n",
    "df_filled_avg=pd.concat([df_numerical_filled_avg,df_pre[\"Rain\"]], axis=1)\n",
    "\n",
    "\n",
    "print(\"DataFrame after replacing null values with the average:\")\n",
    "print(df_filled_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### determine targets & features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_columns=[\"Rain\"]\n",
    "\n",
    "df_targets_filled_avg = df_filled_avg[targets_columns]\n",
    "df_features_filled_avg = df_filled_avg.drop(columns=targets_columns)\n",
    "\n",
    "df_targets_dropped_nulls = df_dropped_nulls[targets_columns]\n",
    "df_features_dropped_nulls = df_dropped_nulls.drop(columns=targets_columns)\n",
    "\n",
    "print(\"Avg data:\")\n",
    "display(df_features_filled_avg.head())\n",
    "display(df_targets_filled_avg.head())\n",
    "\n",
    "print(\"Dropped nulls data:\")\n",
    "display(df_features_dropped_nulls.head())\n",
    "display(df_targets_dropped_nulls.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### splitting data into train , test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make the 80% from the data training set and 20% from the data testing set\n",
    "# random state to ensure that the split return the same data each run\n",
    "\n",
    "df_features_train_avg, df_features_test_avg, df_targets_train_avg, df_targets_test_avg = train_test_split(df_features_filled_avg, df_targets_filled_avg, test_size=0.2, random_state=42) \n",
    "df_features_train_dropped, df_features_test_dropped, df_targets_train_dropped, df_targets_test_dropped = train_test_split(df_features_dropped_nulls, df_targets_dropped_nulls, test_size=0.2, random_state=42) \n",
    "\n",
    "print(len(df_features_train_avg))\n",
    "print(len(df_features_test_avg))\n",
    "print(len(df_targets_train_avg))\n",
    "print(len(df_targets_test_avg))\n",
    "\n",
    "print(len(df_features_train_dropped))\n",
    "print(len(df_features_test_dropped))\n",
    "print(len(df_targets_train_dropped))\n",
    "print(len(df_targets_test_dropped))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final targets will be worked on \n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_targets_train_avg = label_encoder.fit_transform(df_targets_train_avg)\n",
    "df_targets_test_avg = label_encoder.transform(df_targets_test_avg)\n",
    "\n",
    "df_targets_train_dropped = label_encoder.fit_transform(df_targets_train_dropped)\n",
    "df_targets_test_dropped = label_encoder.transform(df_targets_test_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### check scaling of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"filled avg data:\")\n",
    "display(df_filled_avg.describe().T)\n",
    "\n",
    "print(\"dropped nulls data:\")\n",
    "display(df_dropped_nulls.describe().T)\n",
    "\n",
    "# according to the output from min, max the numeric features dosn't have the same scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### features are scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "# the scaler return ndarray\n",
    "\n",
    "df_features_train_avg = scaler.fit_transform(df_features_train_avg)\n",
    "df_features_test_avg = scaler.fit_transform(df_features_test_avg)\n",
    "\n",
    "\n",
    "df_features_train_dropped = scaler.fit_transform(df_features_train_dropped)\n",
    "df_features_test_dropped = scaler.fit_transform(df_features_test_dropped)\n",
    "\n",
    "\n",
    "# convert the ndarray to DataFrame\n",
    "\n",
    "# final features will be worked on\n",
    "\n",
    "df_features_train_avg = pd.DataFrame(df_features_train_avg, columns=df_features_filled_avg.columns)\n",
    "df_features_test_avg = pd.DataFrame(df_features_test_avg, columns=df_features_filled_avg.columns)\n",
    "\n",
    "\n",
    "df_features_train_dropped = pd.DataFrame(df_features_train_dropped, columns=df_features_dropped_nulls.columns)\n",
    "df_features_test_dropped = pd.DataFrame(df_features_test_dropped, columns=df_features_dropped_nulls.columns)\n",
    "\n",
    "\n",
    "print(\"Avg Features:\")\n",
    "display(df_features_train_avg.describe().T)\n",
    "display(df_features_test_avg.describe().T)\n",
    "\n",
    "print(\"Dropped Nulls Features:\")\n",
    "display(df_features_train_dropped.describe().T)\n",
    "display(df_features_test_dropped.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Preprocessing : Implement Decision Tree, k-Nearest Neighbors (kNN) and naÃ¯ve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Evaluate accuracy, precision, and recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluateModels(target, predictions):\n",
    "    # get the percentage \n",
    "    accuracy = accuracy_score(target, predictions) * 100\n",
    "    precision = precision_score(target, predictions) * 100\n",
    "    recall = recall_score(target, predictions) * 100\n",
    "\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\", f\"Precision: {precision:.2f}%\", f\"Recall: {recall:.2f}%\")\n",
    "    return accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### KNN with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# KNN with scikit-learn using 5 Neighbors and brute force\n",
    "knnModel = KNeighborsClassifier(n_neighbors=5, algorithm='brute')\n",
    "\n",
    "# using technique of replacing the nulls values with the mean\n",
    "knnModel.fit(df_features_train_avg, df_targets_train_avg)\n",
    "\n",
    "knnPredictions = knnModel.predict(df_features_test_avg)\n",
    "knn_accuracy_avg, knn_precision_avg, knn_recall_avg = evaluateModels(df_targets_test_avg, knnPredictions)\n",
    "\n",
    "\n",
    "# using technique of dropping the nulls\n",
    "\n",
    "knnModel.fit(df_features_train_dropped, df_targets_train_dropped)\n",
    "knnPredictions = knnModel.predict(df_features_test_dropped)\n",
    "knn_accuracy_dropped, knn_precision_dropped, knn_recall_dropped = evaluateModels(df_targets_test_dropped, knnPredictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### KNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the distances between two points\n",
    "def eculidean_distance(p, q):\n",
    "    distance = 0\n",
    "    for i in range(len(q)):\n",
    "       distance += ( (p[i] - q[i] ) ** 2 )\n",
    "\n",
    "    return np.sqrt(distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### find the neighbors of a point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find the neighbors of a point (x_test)\n",
    "# loop over the x_train to find the neighbors\n",
    "def find_neighbours(x_train, x_test, y_train):\n",
    "    n = len(x_train)\n",
    "    distances = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        distances[i] = eculidean_distance(x_train[i], x_test)\n",
    "\n",
    "\n",
    "    # convert distances and y_train to data frame to can concatenate\n",
    "    distances = pd.DataFrame(distances, columns=['Distance'])\n",
    "    y_train = pd.DataFrame(y_train, columns=['Target'])\n",
    "    neighbours = pd.concat([distances,y_train], axis=1)\n",
    "\n",
    "    # sort the neighbors according to the distances\n",
    "    neighbours = neighbours.sort_values(by='Distance', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    return neighbours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get y predict for a one x test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# take the neighbors and k \n",
    "# Return the value with the highest count\n",
    "def get_y_predict(neighbours, k):\n",
    "    # get first k rows\n",
    "    top_k = neighbours.head(k)\n",
    "\n",
    "    # count the number of 0s and 1s\n",
    "    label_counts = top_k['Target'].value_counts()\n",
    "\n",
    "    # return the value with the highest count\n",
    "    return label_counts.idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get y predict for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# return y predictions for the whole test set\n",
    "def predict(x_train, x_test, y_train, k):\n",
    "    y_predictions = np.zeros(len(x_test))\n",
    "    x_test = x_test.to_numpy()\n",
    "\n",
    "    # loop over the x_test\n",
    "    for i in range(len(x_test)):\n",
    "\n",
    "        # get the neighnours\n",
    "        neighbours = find_neighbours(x_train.to_numpy(), x_test[i], y_train)\n",
    "\n",
    "        # get the y prediction and update the list of predictions\n",
    "        y_predictions[i] = get_y_predict(neighbours, k)\n",
    "\n",
    "    return y_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### different k values for KNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN from Scratch with k = 3\n",
    "knn_scratch_predictions = predict(df_features_train_avg, df_features_test_avg, df_targets_train_avg, 3)\n",
    "print(\"KNN FROM SCRATCH WITH K = 3\")\n",
    "knn3_scratch_accuracy, knn3_scratch_precision, knn3_scratch_recall = evaluateModels(df_targets_test_avg, knn_scratch_predictions)\n",
    "\n",
    "# KNN from Scratch with k = 5\n",
    "knn_scratch_predictions = predict(df_features_train_avg, df_features_test_avg, df_targets_train_avg, 5)\n",
    "print(\"KNN FROM SCRATCH WITH K = 5\")\n",
    "knn5_scratch_accuracy, knn5_scratch_precision, knn5_scratch_recall = evaluateModels(df_targets_test_avg, knn_scratch_predictions)\n",
    "\n",
    "# KNN from Scratch with k = 7\n",
    "knn_scratch_predictions = predict(df_features_train_avg, df_features_test_avg, df_targets_train_avg, 7)\n",
    "print(\"KNN FROM SCRATCH WITH K = 7\")\n",
    "knn7_scratch_accuracy, knn7_scratch_precision, knn7_scratch_recall = evaluateModels(df_targets_test_avg, knn_scratch_predictions)\n",
    "\n",
    "# KNN from Scratch with k = 9\n",
    "knn_scratch_predictions = predict(df_features_train_avg, df_features_test_avg, df_targets_train_avg, 9)\n",
    "print(\"KNN FROM SCRATCH WITH K = 9\")\n",
    "knn9_scratch_accuracy, knn9_scratch_precision, knn9_scratch_recall = evaluateModels(df_targets_test_avg, knn_scratch_predictions)\n",
    "\n",
    "# KNN from Scratch with k = 11\n",
    "knn_scratch_predictions = predict(df_features_train_avg, df_features_test_avg, df_targets_train_avg, 11)\n",
    "print(\"KNN FROM SCRATCH WITH K = 11\")\n",
    "knn11_scratch_accuracy, knn11_scratch_precision, knn11_scratch_recall = evaluateModels(df_targets_test_avg, knn_scratch_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
