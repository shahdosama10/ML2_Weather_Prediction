{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### load 'weather_forecast_data.csv' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('weather_forecast_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get copy from the original to preprocess\n",
    "\n",
    "df_pre = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to know the number of the rows\n",
    "print(f\"total records:\",len(df), \"\\n\")\n",
    "\n",
    "\n",
    "# to get the number of missing values in each column\n",
    "print(\"missing records in each column:\",\"\\n\")\n",
    "print(df_pre.isnull().sum())\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"Records with null values: \",\"\\n\")\n",
    "print(df_pre[df_pre.isnull().any(axis=1)])\n",
    "\n",
    "# according to the output there are missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Handle missing values with dropping them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dropped_nulls= df_pre.dropna()\n",
    "print(f\"total records without nulls:\",len(df_dropped_nulls), \"\\n\")\n",
    "\n",
    "df_dropped_nulls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Handle missing values with replacing them with Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the numerical features only because we can't get mean for categorical feature\n",
    "\n",
    "df_numerical_features_only=df_pre.select_dtypes(include=\"number\")\n",
    "\n",
    "\n",
    "# replace the null values with the average of the numerical features\n",
    "\n",
    "df_numerical_filled_avg = df_numerical_features_only.fillna(df_numerical_features_only.mean())\n",
    "\n",
    "\n",
    "# concatenate the numerical features with the target column \"Rain\" and create a new dataframe \"df_filledAvg\"\n",
    "\n",
    "df_filled_avg=pd.concat([df_numerical_filled_avg,df_pre[\"Rain\"]], axis=1)\n",
    "\n",
    "\n",
    "print(\"DataFrame after replacing null values with the average:\")\n",
    "print(df_filled_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### determine targets & features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_columns=[\"Rain\"]\n",
    "\n",
    "df_targets_filled_avg = df_filled_avg[targets_columns]\n",
    "df_features_filled_avg = df_filled_avg.drop(columns=targets_columns)\n",
    "\n",
    "df_targets_dropped_nulls = df_dropped_nulls[targets_columns]\n",
    "df_features_dropped_nulls = df_dropped_nulls.drop(columns=targets_columns)\n",
    "\n",
    "print(\"Avg data:\")\n",
    "display(df_features_filled_avg.head())\n",
    "display(df_targets_filled_avg.head())\n",
    "\n",
    "print(\"Dropped nulls data:\")\n",
    "display(df_features_dropped_nulls.head())\n",
    "display(df_targets_dropped_nulls.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### splitting data into train , test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make the 80% from the data training set and 20% from the data testing set\n",
    "# random state to ensure that the split return the same data each run\n",
    "\n",
    "df_features_train_avg, df_features_test_avg, df_targets_train_avg, df_targets_test_avg = train_test_split(df_features_filled_avg, df_targets_filled_avg, test_size=0.2, random_state=42) \n",
    "df_features_train_dropped, df_features_test_dropped, df_targets_train_dropped, df_targets_test_dropped = train_test_split(df_features_dropped_nulls, df_targets_dropped_nulls, test_size=0.2, random_state=42) \n",
    "\n",
    "print(len(df_features_train_avg))\n",
    "print(len(df_features_test_avg))\n",
    "print(len(df_targets_train_avg))\n",
    "print(len(df_targets_test_avg))\n",
    "\n",
    "print(len(df_features_train_dropped))\n",
    "print(len(df_features_test_dropped))\n",
    "print(len(df_targets_train_dropped))\n",
    "print(len(df_targets_test_dropped))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final targets will be worked on \n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_targets_train_avg = label_encoder.fit_transform(df_targets_train_avg)\n",
    "df_targets_test_avg = label_encoder.transform(df_targets_test_avg)\n",
    "\n",
    "df_targets_train_dropped = label_encoder.fit_transform(df_targets_train_dropped)\n",
    "df_targets_test_dropped = label_encoder.transform(df_targets_test_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### check scaling of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"filled avg data:\")\n",
    "display(df_filled_avg.describe().T)\n",
    "\n",
    "print(\"dropped nulls data:\")\n",
    "display(df_dropped_nulls.describe().T)\n",
    "\n",
    "# according to the output from min, max the numeric features dosn't have the same scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### features are scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "# the scaler return ndarray\n",
    "\n",
    "df_features_train_avg = scaler.fit_transform(df_features_train_avg)\n",
    "df_features_test_avg = scaler.fit_transform(df_features_test_avg)\n",
    "\n",
    "\n",
    "df_features_train_dropped = scaler.fit_transform(df_features_train_dropped)\n",
    "df_features_test_dropped = scaler.fit_transform(df_features_test_dropped)\n",
    "\n",
    "\n",
    "# convert the ndarray to DataFrame\n",
    "\n",
    "# final features will be worked on\n",
    "\n",
    "df_features_train_avg = pd.DataFrame(df_features_train_avg, columns=df_features_filled_avg.columns)\n",
    "df_features_test_avg = pd.DataFrame(df_features_test_avg, columns=df_features_filled_avg.columns)\n",
    "\n",
    "\n",
    "df_features_train_dropped = pd.DataFrame(df_features_train_dropped, columns=df_features_dropped_nulls.columns)\n",
    "df_features_test_dropped = pd.DataFrame(df_features_test_dropped, columns=df_features_dropped_nulls.columns)\n",
    "\n",
    "\n",
    "print(\"Avg Features:\")\n",
    "display(df_features_train_avg.describe().T)\n",
    "display(df_features_test_avg.describe().T)\n",
    "\n",
    "print(\"Dropped Nulls Features:\")\n",
    "display(df_features_train_dropped.describe().T)\n",
    "display(df_features_test_dropped.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Decision Tree, k-Nearest Neighbors (kNN) and naÃ¯ve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Evaluate accuracy, precision, and recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluateModels(target, predictions):\n",
    "    # get the percentage \n",
    "    accuracy = accuracy_score(target, predictions) * 100\n",
    "    precision = precision_score(target, predictions) * 100\n",
    "    recall = recall_score(target, predictions) * 100\n",
    "\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\", f\"Precision: {precision:.2f}%\", f\"Recall: {recall:.2f}%\")\n",
    "    return accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### KNN with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# KNN with scikit-learn using 5 Neighbors and brute force\n",
    "knnModel = KNeighborsClassifier(n_neighbors=5, algorithm='brute')\n",
    "\n",
    "# using technique of replacing the nulls values with the mean\n",
    "knnModel.fit(df_features_train_avg, df_targets_train_avg)\n",
    "\n",
    "knnPredictions = knnModel.predict(df_features_test_avg)\n",
    "print(\"KNN using technique of replacing the nulls values with the mean \")\n",
    "knn_accuracy_avg, knn_precision_avg, knn_recall_avg = evaluateModels(df_targets_test_avg, knnPredictions)\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "# using technique of dropping the nulls\n",
    "print(\"KNN using technique of dropping the nulls\")\n",
    "\n",
    "knnModel.fit(df_features_train_dropped, df_targets_train_dropped)\n",
    "knnPredictions = knnModel.predict(df_features_test_dropped)\n",
    "knn_accuracy_dropped, knn_precision_dropped, knn_recall_dropped = evaluateModels(df_targets_test_dropped, knnPredictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### KNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the distances between two points\n",
    "def eculidean_distance(p, q):\n",
    "    distance = 0\n",
    "    for i in range(len(q)):\n",
    "       distance += ( (p[i] - q[i] ) ** 2 )\n",
    "\n",
    "    return np.sqrt(distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### find the neighbors of a point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find the neighbors of a point (x_test)\n",
    "# loop over the x_train to find the neighbors\n",
    "def find_neighbours(x_train, x_test, y_train):\n",
    "    n = len(x_train)\n",
    "    distances = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        distances[i] = eculidean_distance(x_train[i], x_test)\n",
    "\n",
    "\n",
    "    # convert distances and y_train to data frame to can concatenate\n",
    "    distances = pd.DataFrame(distances, columns=['Distance'])\n",
    "    y_train = pd.DataFrame(y_train, columns=['Target'])\n",
    "    neighbours = pd.concat([distances,y_train], axis=1)\n",
    "\n",
    "    # sort the neighbors according to the distances\n",
    "    neighbours = neighbours.sort_values(by='Distance', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    return neighbours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### get y predict for a one x test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# take the neighbors and k \n",
    "# Return the value with the highest count\n",
    "def get_y_predict(neighbours, k):\n",
    "    # get first k rows\n",
    "    top_k = neighbours.head(k)\n",
    "\n",
    "    # count the number of 0s and 1s\n",
    "    label_counts = top_k['Target'].value_counts()\n",
    "\n",
    "    # return the value with the highest count\n",
    "    return label_counts.idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### get y predict for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# return y predictions for the whole test set\n",
    "def predict(x_train, x_test, y_train, k):\n",
    "    y_predictions = np.zeros(len(x_test))\n",
    "    x_test = x_test.to_numpy()\n",
    "\n",
    "    # loop over the x_test\n",
    "    for i in range(len(x_test)):\n",
    "\n",
    "        # get the neighnours\n",
    "        neighbours = find_neighbours(x_train.to_numpy(), x_test[i], y_train)\n",
    "\n",
    "        # get the y prediction and update the list of predictions\n",
    "        y_predictions[i] = get_y_predict(neighbours, k)\n",
    "\n",
    "    return y_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### different k values for KNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN from Scratch with k = 3\n",
    "knn_scratch_predictions = predict(df_features_train_avg, df_features_test_avg, df_targets_train_avg, 3)\n",
    "print(\"KNN FROM SCRATCH WITH K = 3\")\n",
    "knn3_scratch_accuracy, knn3_scratch_precision, knn3_scratch_recall = evaluateModels(df_targets_test_avg, knn_scratch_predictions)\n",
    "print(\"-\"*80)\n",
    "\n",
    "# KNN from Scratch with k = 5\n",
    "knn_scratch_predictions = predict(df_features_train_avg, df_features_test_avg, df_targets_train_avg, 5)\n",
    "print(\"KNN FROM SCRATCH WITH K = 5\")\n",
    "knn5_scratch_accuracy, knn5_scratch_precision, knn5_scratch_recall = evaluateModels(df_targets_test_avg, knn_scratch_predictions)\n",
    "print(\"-\"*80)\n",
    "\n",
    "# KNN from Scratch with k = 7\n",
    "knn_scratch_predictions = predict(df_features_train_avg, df_features_test_avg, df_targets_train_avg, 7)\n",
    "print(\"KNN FROM SCRATCH WITH K = 7\")\n",
    "knn7_scratch_accuracy, knn7_scratch_precision, knn7_scratch_recall = evaluateModels(df_targets_test_avg, knn_scratch_predictions)\n",
    "print(\"-\"*80)\n",
    "\n",
    "# KNN from Scratch with k = 9\n",
    "knn_scratch_predictions = predict(df_features_train_avg, df_features_test_avg, df_targets_train_avg, 9)\n",
    "print(\"KNN FROM SCRATCH WITH K = 9\")\n",
    "knn9_scratch_accuracy, knn9_scratch_precision, knn9_scratch_recall = evaluateModels(df_targets_test_avg, knn_scratch_predictions)\n",
    "print(\"-\"*80)\n",
    "\n",
    "# KNN from Scratch with k = 11\n",
    "knn_scratch_predictions = predict(df_features_train_avg, df_features_test_avg, df_targets_train_avg, 11)\n",
    "print(\"KNN FROM SCRATCH WITH K = 11\")\n",
    "knn11_scratch_accuracy, knn11_scratch_precision, knn11_scratch_recall = evaluateModels(df_targets_test_avg, knn_scratch_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Implement Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model_avg = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "dt_model_avg.fit(df_features_train_avg, df_targets_train_avg)  #usign averge filled\n",
    "\n",
    "dt_preds_avg = dt_model_avg.predict(df_features_test_avg)\n",
    "\n",
    "print(\"Decision Tree usign averge filled\")\n",
    "dt_accuracy_avg , dt_precision_avg , dt_recall_avg = evaluateModels(df_targets_test_avg , dt_preds_avg)\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "dt_model_dropped = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "\n",
    "dt_model_dropped.fit(df_features_train_dropped, df_targets_train_dropped) # using dropped nulls\n",
    "\n",
    "dt_preds_dropped = dt_model_dropped.predict(df_features_test_dropped)\n",
    "\n",
    "print(\"Decision Tree usign dropped nulls\")\n",
    "dt_accuracy_dropped , dt_precision_dropped , dt_recall_dropped = evaluateModels(df_targets_test_dropped , dt_preds_dropped)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Decision Tree Explanation Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">##### plot of the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Avgerage-Filled\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    dt_model_avg, \n",
    "    feature_names=df_features_filled_avg.columns, \n",
    "    class_names=[\"No Rain\", \"Rain\"], \n",
    "    filled=True, \n",
    "    rounded=True\n",
    ")\n",
    "plt.title(\"Decision Tree\")\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the classifer\n",
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Model the features with avg replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(df_features_train_avg, df_targets_train_avg)\n",
    "\n",
    "# predict the target values\n",
    "y_pred_avg = model.predict(df_features_test_avg)\n",
    "\n",
    "# evaluate the target values\n",
    "print(\"Avg data:\")\n",
    "\n",
    "nb_accuracy_avg , nb_precision_avg , nb_recall_avg = evaluateModels(df_targets_test_avg ,y_pred_avg )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Model the features with dropped nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(df_features_train_dropped , df_targets_train_dropped)\n",
    "\n",
    "# predict the target values\n",
    "y_pred_dropped = model.predict(df_features_test_dropped)\n",
    "\n",
    "# evaluate the target values\n",
    "print(\"Dropped Nulls data:\")\n",
    "\n",
    "nb_accuracy_dropped , nb_precision_dropped , nb_recall_dropped = evaluateModels(df_targets_test_dropped ,y_pred_dropped )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the data\n",
    "\n",
    "import seaborn as sns\n",
    "sns.displot(df_features_test_avg)\n",
    "sns.displot(df_features_test_dropped)\n",
    "\n",
    "# The features are not normally distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(df_targets_test_avg, y_pred_avg))\n",
    "# print(classification_report(df_targets_test_dropped, y_pred_dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare between Decision Tree, k-Nearest Neighbors (kNN) and naÃ¯ve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Compare the performance of your implementations by evaluating accuracy, precision, and recall metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = {\n",
    "    \"Model\": [\n",
    "        \"KNN(K=5) \",\n",
    "        \"Decision Tree \",\n",
    "        \"Naive Bayes \"\n",
    "    ],\n",
    "    \"Accuracy (%)\": [knn_accuracy_avg, dt_accuracy_avg, nb_accuracy_avg],\n",
    "    \"Precision (%)\": [knn_precision_avg, dt_precision_avg, nb_precision_avg],\n",
    "    \"Recall (%)\": [knn_recall_avg, dt_recall_avg, nb_recall_avg]\n",
    "}\n",
    "\n",
    "\n",
    "comparison_table = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\nComparison of Models (Replacing Nulls with Mean)\\n\")\n",
    "print(comparison_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### compare the performance of your custom kNN implementation with the pre-built kNN algorithms in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = {\n",
    "    \"Model\": [\n",
    "        \"KNN (Scratch Implementation)\",\n",
    "        \"KNN (Library Implementation)\"\n",
    "    ],\n",
    "    \"Accuracy (%)\": [knn5_scratch_accuracy, knn_accuracy_avg],\n",
    "    \"Precision (%)\": [knn5_scratch_precision, knn_precision_avg],\n",
    "    \"Recall (%)\": [knn5_scratch_recall, knn_recall_avg]\n",
    "}\n",
    "\n",
    "comparison_table = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\nComparison of KNN Implementations\\n\")\n",
    "print(comparison_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### evaluating the performance of scikit learn implementations of 3 algorithms with respect to the different handling missing data technique. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Model\": [\n",
    "        \"KNN(K=5) \",\n",
    "        \"Decision Tree \",\n",
    "        \"Naive Bayes \"\n",
    "    ],\n",
    "    \"Accuracy (%)\": [knn_accuracy_avg, dt_accuracy_avg, nb_accuracy_avg],\n",
    "    \"Precision (%)\": [knn_precision_avg, dt_precision_avg, nb_precision_avg],\n",
    "    \"Recall (%)\": [knn_recall_avg, dt_recall_avg, nb_recall_avg]\n",
    "}\n",
    "\n",
    "\n",
    "comparison_table = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\nComparison of Models (Replacing Nulls with Mean)\\n\")\n",
    "print(comparison_table)\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"Model\": [\n",
    "        \"KNN(K=5) \",\n",
    "        \"Decision Tree \",\n",
    "        \"Naive Bayes \"\n",
    "    ],\n",
    "    \"Accuracy (%)\": [knn_accuracy_dropped, dt_accuracy_dropped, nb_accuracy_dropped],\n",
    "    \"Precision (%)\": [knn_precision_dropped, dt_precision_dropped, nb_precision_dropped],\n",
    "    \"Recall (%)\": [knn_recall_dropped, dt_recall_dropped, nb_recall_dropped]\n",
    "}\n",
    "\n",
    "\n",
    "comparison_table = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\nComparison of Models (Dropping Nulls)\\n\")\n",
    "print(comparison_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### evaluating the performance of your implementations of the k-Nearest Neighbors (kNN) from scratch with different k values at least 5 values. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Compare these results with the performance of the corresponding algorithms implemented using scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {\n",
    "    \"Model\": [\n",
    "        \"KNN (Scratch, K=3)\",\n",
    "        \"KNN (Scratch, K=5)\",\n",
    "        \"KNN (Scratch, K=7)\",\n",
    "        \"KNN (Scratch, K=9)\",\n",
    "        \"KNN (Scratch, K=11)\",\n",
    "        \"KNN (scikit-learn, K=5)\",\n",
    "        \"Decision Tree\",\n",
    "        \"Naive Bayes\"\n",
    "      \n",
    "    ],\n",
    "    \"Accuracy (%)\": [\n",
    "        knn3_scratch_accuracy, knn5_scratch_accuracy,\n",
    "        knn7_scratch_accuracy, knn9_scratch_accuracy,\n",
    "        knn11_scratch_accuracy,\n",
    "        knn_accuracy_avg, dt_accuracy_avg, nb_accuracy_avg\n",
    "       \n",
    "    ],\n",
    "    \"Precision (%)\": [\n",
    "        knn3_scratch_precision, knn5_scratch_precision,\n",
    "        knn7_scratch_precision, knn9_scratch_precision,\n",
    "        knn11_scratch_precision,\n",
    "        knn_precision_avg, dt_precision_avg, nb_precision_avg\n",
    "      \n",
    "    ],\n",
    "    \"Recall (%)\": [\n",
    "        knn3_scratch_recall, knn5_scratch_recall,\n",
    "        knn7_scratch_recall, knn9_scratch_recall,\n",
    "        knn11_scratch_recall,\n",
    "        knn_recall_avg, dt_recall_avg, nb_recall_avg\n",
    "        \n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_table = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\nComparison of Models (Including KNN from Scratch with Various K Values)\\n\")\n",
    "print(comparison_table)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
