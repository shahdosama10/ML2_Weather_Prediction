{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### load 'weather_forecast_data.csv' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('weather_forecast_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get copy from the original to preprocess\n",
    "\n",
    "df_pre = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to know the number of the rows\n",
    "print(f\"total records:\",len(df), \"\\n\")\n",
    "\n",
    "\n",
    "# to get the number of missing values in each column\n",
    "print(\"missing records in each column:\",\"\\n\")\n",
    "print(df_pre.isnull().sum())\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"Records with null values: \",\"\\n\")\n",
    "print(df_pre[df_pre.isnull().any(axis=1)])\n",
    "\n",
    "# according to the output there are missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Handle missing values with dropping them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dropped_nulls= df_pre.dropna()\n",
    "print(f\"total records without nulls:\",len(df_dropped_nulls), \"\\n\")\n",
    "\n",
    "df_dropped_nulls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Handle missing values with replacing them with Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the numerical features only because we can't get mean for categorical feature\n",
    "\n",
    "df_numerical_features_only=df_pre.select_dtypes(include=\"number\")\n",
    "\n",
    "\n",
    "# replace the null values with the average of the numerical features\n",
    "\n",
    "df_numerical_filled_avg = df_numerical_features_only.fillna(df_numerical_features_only.mean())\n",
    "\n",
    "\n",
    "# concatenate the numerical features with the target column \"Rain\" and create a new dataframe \"df_filledAvg\"\n",
    "\n",
    "df_filled_avg=pd.concat([df_numerical_filled_avg,df_pre[\"Rain\"]], axis=1)\n",
    "\n",
    "\n",
    "print(\"DataFrame after replacing null values with the average:\")\n",
    "print(df_filled_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### determine targets & features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_columns=[\"Rain\"]\n",
    "\n",
    "df_targets_filled_avg = df_filled_avg[targets_columns]\n",
    "df_features_filled_avg = df_filled_avg.drop(columns=targets_columns)\n",
    "\n",
    "df_targets_dropped_nulls = df_dropped_nulls[targets_columns]\n",
    "df_features_dropped_nulls = df_dropped_nulls.drop(columns=targets_columns)\n",
    "\n",
    "print(\"Avg data:\")\n",
    "display(df_features_filled_avg.head())\n",
    "display(df_targets_filled_avg.head())\n",
    "\n",
    "print(\"Dropped nulls data:\")\n",
    "display(df_features_dropped_nulls.head())\n",
    "display(df_targets_dropped_nulls.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### splitting data into train , test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make the 80% from the data training set and 20% from the data testing set\n",
    "# random state to ensure that the split return the same data each run\n",
    "\n",
    "# final targets will be worked on \n",
    "\n",
    "df_features_train_avg, df_features_test_avg, df_targets_train_avg, df_targets_test_avg = train_test_split(df_features_filled_avg, df_targets_filled_avg, test_size=0.2, random_state=42) \n",
    "df_features_train_dropped, df_features_test_dropped, df_targets_train_dropped, df_targets_test_dropped = train_test_split(df_features_dropped_nulls, df_targets_dropped_nulls, test_size=0.2, random_state=42) \n",
    "\n",
    "print(len(df_features_train_avg))\n",
    "print(len(df_features_test_avg))\n",
    "print(len(df_targets_train_avg))\n",
    "print(len(df_targets_test_avg))\n",
    "\n",
    "print(len(df_features_train_dropped))\n",
    "print(len(df_features_test_dropped))\n",
    "print(len(df_targets_train_dropped))\n",
    "print(len(df_targets_test_dropped))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### check scaling of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"filled avg data:\")\n",
    "display(df_filled_avg.describe().T)\n",
    "\n",
    "print(\"dropped nulls data:\")\n",
    "display(df_dropped_nulls.describe().T)\n",
    "\n",
    "# according to the output from min, max the numeric features dosn't have the same scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### features are scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "# the scaler return ndarray\n",
    "\n",
    "df_features_train_avg = scaler.fit_transform(df_features_train_avg)\n",
    "df_features_test_avg = scaler.fit_transform(df_features_test_avg)\n",
    "\n",
    "\n",
    "df_features_train_dropped = scaler.fit_transform(df_features_train_dropped)\n",
    "df_features_test_dropped = scaler.fit_transform(df_features_test_dropped)\n",
    "\n",
    "\n",
    "# convert the ndarray to DataFrame\n",
    "\n",
    "# final features will be worked on\n",
    "\n",
    "df_features_train_avg = pd.DataFrame(df_features_train_avg, columns=df_features_filled_avg.columns)\n",
    "df_features_test_avg = pd.DataFrame(df_features_test_avg, columns=df_features_filled_avg.columns)\n",
    "\n",
    "\n",
    "df_features_train_dropped = pd.DataFrame(df_features_train_dropped, columns=df_features_dropped_nulls.columns)\n",
    "df_features_test_dropped = pd.DataFrame(df_features_test_dropped, columns=df_features_dropped_nulls.columns)\n",
    "\n",
    "\n",
    "print(\"Avg Features:\")\n",
    "display(df_features_train_avg.describe().T)\n",
    "display(df_features_test_avg.describe().T)\n",
    "\n",
    "print(\"Dropped Nulls Features:\")\n",
    "display(df_features_train_dropped.describe().T)\n",
    "display(df_features_test_dropped.describe().T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
